{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ac7514",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9cf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf97697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    # 多头注意力 解码内部的自注意力机制\n",
    "    self.attention1 = MultiHeadAttention(d_model, n_head)\n",
    "    self.norm1 = LayerNorm(d_model)\n",
    "    self.dropout1 = nn.Dropout(drop_prob)\n",
    "    # 多头注意力 编码器-解码器注意力机制\n",
    "    # 跨模态注意力机制:encoder编码之后输入到decoder中\n",
    "    self.cross_attention = MultiHeadAttention(d_model, n_head)\n",
    "    self.norm2 = LayerNorm(d_model)\n",
    "    self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
    "    self.norm3 = LayerNorm(d_model)\n",
    "    self.dropout3 = nn.Dropout(drop_prob)\n",
    "  \"\"\"\n",
    "    解码器层的前向传播\n",
    "    :dec_output: 解码器的输入 (batch_size, seq_len, d_model)\n",
    "    :enc_output: 编码器的输出 (batch_size, seq_len, d_model)\n",
    "    :t_mask: 解码器自注意力机制的掩码 (batch_size, seq_len, seq_len)\n",
    "    :s_mask: 编码器-解码器注意力机制的掩码 (batch_size, seq_len, seq_len)\n",
    "  \"\"\"\n",
    "  def forward(self, dec_output, enc_output, t_mask=None, s_mask=None):\n",
    "    # 解码器自注意力机制\n",
    "    _x = dec_output\n",
    "    x = self.attention1(dec_output, dec_output, dec_output, t_mask)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.norm1(x + _x)\n",
    "    _x = x\n",
    "    # 编码器-解码器注意力机制 跨模态的注意力(交叉注意力)\n",
    "    x = self.cross_attention(x, enc_output, enc_output, s_mask)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.norm2(x + _x)\n",
    "    # 前馈神经网络\n",
    "    x = self.ffn(x)\n",
    "    x = self.dropout3(ffn_output)\n",
    "    x = self.norm3(x + _x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf98374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  \"\"\"\n",
    "    解码器\n",
    "    :dec_voc_size: 解码器词表大小\n",
    "    :max_len: 序列最大长度  \n",
    "    :d_model: 词向量维度\n",
    "    :ffn_hidden: 前馈神经网络隐藏层维度\n",
    "    :n_head: 多头注意力机制头数\n",
    "    :n_layers: 解码器层数\n",
    "    :drop_prob: dropout概率\n",
    "  \"\"\"\n",
    "  def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.embedding = TransformerEmbedding(dec_voc_size, d_model, max_len, drop_prob, device)\n",
    "    self.layers = nn.ModuleList([\n",
    "      DecoderLayer(d_model, ffn_hidden, n_head, drop_prob)\n",
    "      for _ in range(n_layers)\n",
    "    ])\n",
    "    self.fc = nn.Linear(d_model, dec_voc_size)\n",
    "  # 目标掩码和源掩码\n",
    "  def forward(self, dec, enc, t_mask=None, s_mask=None):\n",
    "    # 解码器嵌入层\n",
    "    dec = self.embedding(enc)\n",
    "    # 多层解码器层\n",
    "    for layer in self.layers:\n",
    "      dec = layer(dec, enc, t_mask, s_mask)\n",
    "    # 线性映射到词表大小\n",
    "    dec = self.fc(dec)\n",
    "    return dec  # 返回整个词汇表概率的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269cc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71600538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a2b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1daddf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9616f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
