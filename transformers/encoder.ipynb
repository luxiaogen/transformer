{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11d5275",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "## Feed Forward前馈神经网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba3add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2288b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "      :d_model 输入输出维度 特征维度\n",
    "      :hidden 前馈神经网络隐藏层维度\n",
    "      :dropout dropout比率 正则化的一个基础,防止过拟合\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, hidden, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)   # 防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b3d4a",
   "metadata": {},
   "source": [
    "## EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90243e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, ffn_hidden, n_head, dropout=0.1):\n",
    "    self.attention = MultiHeadAttention(d_model, n_head)\n",
    "    self.norm1 = LayerNorm(d_model)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, dropout)\n",
    "    self.norm2 = LayerNorm(d_model)\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x, mask=None):\n",
    "    # Multi-Head Attention\n",
    "    _x = x # 暂存一下原始x\n",
    "    x = self.attention(x, x, x, mask) # mask 考虑忽略掉某些位置\n",
    "    x = self.dropout1(x)\n",
    "    x = self.norm1(x + _x) # 残差连接\n",
    "    _x = x # 再次暂存第一部分的输出\n",
    "\n",
    "    # Feed Forward\n",
    "    x = self.ffn(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.norm2(x + _x) # 残差连接\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  \"\"\"\n",
    "  :enc_voc: 输入词表大小\n",
    "  :max_len: 输入序列最大长度\n",
    "  :d_model: 特征维度\n",
    "  :ffn_hidden: 前馈神经网络隐藏层维度\n",
    "  :n_head: 多头注意力机制头数\n",
    "  :n_layers: 编码器层数\n",
    "  :dropout: dropout比率\n",
    "  :device: 设备\n",
    "  \"\"\"\n",
    "  def __init__(self, enc_voc_size, max_len, d_model, \\\n",
    "    ffn_hidden, n_head, n_layers, dropout=0.1, device):\n",
    "    self.embedding = TransformerEmbedding(enc_voc_size, d_model, max_len, dropout, device) # 映射到高维空间\n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        EncoderLayer(d_model, ffn_hidden, n_head, dropout) \n",
    "        for _ in range(n_layers)\n",
    "      ]\n",
    "    )\n",
    "  \n",
    "  def forward(self, x, s_mask):\n",
    "    # x: (batch_size, seq_len) 从词汇表中的索引映射到高维空间\n",
    "    x = self.embedding(x) # (batch_size, seq_len, d_model)\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, s_mask) # (batch_size, seq_len, d_model)\n",
    "    return x # 返回所有编码器处理后的x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42223ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d42a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f80e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77333546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
